{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WgeXrK9nYjaHAUzMUNgcbCjBfi6u0r0f",
      "authorship_tag": "ABX9TyNrqEQ6RziCm3D/myNW3SNw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanhengHe/BigDataCourseProj/blob/main/bigdataproj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgMFKRmRkdmq",
        "outputId": "0f11156a-f5fb-435c-8a69-930181dce9af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=False)\n",
        "\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/Dataset_small/data1.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"targetdir\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/targetdir/data1/train/filename.ext\", \"r\") as file:\n",
        "  pass"
      ],
      "metadata": {
        "id": "U6jEzqxzEy4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.cuda.current_device()\n",
        "torch.cuda._initialized = True\n",
        "\n",
        "\n",
        "class Inception(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1_b1 = nn.Conv2d(256, 256, 1)\n",
        "        self.conv2_b1 = nn.Conv2d(256, 320, 3, padding=1)\n",
        "        self.conv3_b1 = nn.Conv2d(320, 320, 3, padding=1)\n",
        "\n",
        "        self.conv1_b2 = nn.Conv2d(256, 48, 1)\n",
        "        self.conv2_b2 = nn.Conv2d(48, 64, (1, 5), padding=(0, 2))\n",
        "        self.conv3_b2 = nn.Conv2d(64, 64, (5, 1), padding=(2, 0))\n",
        "\n",
        "        self.conv1_b3 = nn.Conv2d(256, 64, 1)\n",
        "\n",
        "        self.conv1_b4 = nn.Conv2d(256, 64, 1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # branch 1\n",
        "        b1 = self.conv1_b1(X)\n",
        "        b1 = b1.mul(torch.sigmoid(b1))\n",
        "\n",
        "        b1 = self.conv2_b1(b1)\n",
        "        b1 = b1.mul(torch.sigmoid(b1))\n",
        "\n",
        "        b1 = self.conv3_b1(b1)\n",
        "        b1 = b1.mul(torch.sigmoid(b1))\n",
        "\n",
        "        # branch 2\n",
        "        b2 = self.conv1_b2(X)\n",
        "        b2 = b2.mul(torch.sigmoid(b2))\n",
        "\n",
        "        b2 = self.conv2_b2(b2)\n",
        "        b2 = b2.mul(torch.sigmoid(b2))\n",
        "\n",
        "        b2 = self.conv3_b2(b2)\n",
        "        b2 = b2.mul(torch.sigmoid(b2))\n",
        "\n",
        "        # branch 3\n",
        "        b3 = self.conv1_b3(X)\n",
        "        b3 = b3.mul(torch.sigmoid(b3))\n",
        "\n",
        "        # branch 4\n",
        "        b4 = F.max_pool2d(X, (3, 3), stride=1, padding=1)  # 2*2 的pooling拼不上\n",
        "        b4 = self.conv1_b4(b4)\n",
        "        b4 = b4.mul(torch.sigmoid(b4))\n",
        "\n",
        "        return torch.cat([b1, b2, b3, b4], 1)\n",
        "\n",
        "\n",
        "class LeNet_II(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # convolutional layer\n",
        "        self.conv1_b1 = nn.Conv2d(1, 64, 3, padding=1)\n",
        "        self.conv2_b1 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv3_b1 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "\n",
        "        self.conv1_b2 = nn.Conv2d(1, 64, 3, padding=2, dilation=2)\n",
        "        self.conv2_b2 = nn.Conv2d(64, 128, 3, padding=2, dilation=2)\n",
        "        self.conv3_b2 = nn.Conv2d(128, 256, 3, padding=2, dilation=2)\n",
        "        self.conv4_b2 = nn.Conv2d(256, 512, 3, padding=2, dilation=2)\n",
        "\n",
        "        self.conv = nn.Conv2d(1024, 768, 3, padding=1)\n",
        "\n",
        "        # full connect layer\n",
        "        self.fc1 = nn.Linear(768 * 4 * 4, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 3926)\n",
        "\n",
        "        # Inception \n",
        "        self.Inception = Inception()\n",
        "\n",
        "    # forward propagation\n",
        "    def forward(self, X):\n",
        "        # branch 1\n",
        "        b1 = self.conv1_b1(X)\n",
        "        b1 = F.max_pool2d(b1, (2, 2), stride=2)\n",
        "        b1 = b1.mul(torch.sigmoid(b1))\n",
        "\n",
        "        b1 = self.conv2_b1(b1)\n",
        "        b1 = F.max_pool2d(b1, (2, 2), stride=2)\n",
        "        b1 = b1.mul(torch.sigmoid(b1))\n",
        "\n",
        "        b1 = self.conv3_b1(b1)\n",
        "        b1 = F.max_pool2d(b1, (2, 2), stride=2)\n",
        "        b1 = b1.mul(torch.sigmoid(b1))\n",
        "\n",
        "        b1 = self.Inception(b1)\n",
        "\n",
        "        # branch 2\n",
        "        b2 = self.conv1_b2(X)\n",
        "        b2 = F.max_pool2d(b2, (2, 2), stride=2)\n",
        "        b2 = b2.mul(torch.sigmoid(b2))\n",
        "\n",
        "        b2 = self.conv2_b2(b2)\n",
        "        b2 = F.max_pool2d(b2, (2, 2), stride=2)\n",
        "        b2 = b2.mul(torch.sigmoid(b2))\n",
        "\n",
        "        b2 = self.conv3_b2(b2)\n",
        "        b2 = b2.mul(torch.sigmoid(b2))\n",
        "\n",
        "        b2 = self.conv4_b2(b2)\n",
        "        b2 = F.max_pool2d(b2, (2, 2), stride=2)\n",
        "        b2 = b2.mul(torch.sigmoid(b2))\n",
        "\n",
        "        # tail\n",
        "        out = self.conv(torch.cat([b1, b2], 1))\n",
        "        out = F.max_pool2d(out, (2, 2), stride=2)\n",
        "        out = out.mul(torch.sigmoid(out))\n",
        "        # unfold\n",
        "        out = torch.flatten(out, start_dim=1)\n",
        "\n",
        "        # full connect\n",
        "        out = self.fc1(out)\n",
        "        out = out.mul(torch.sigmoid(out))\n",
        "        out = F.dropout(out)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "        out = F.log_softmax(out, dim=-1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# # test\n",
        "# ran = torch.rand((1, 1, 64, 64))\n",
        "# model = LeNet_II()\n",
        "# model(ran)\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# dataMatrix = np.load('./dataMatrix.npy')\n",
        "labels = np.load('./labels_num.npy')\n",
        "\n",
        "rate = 0.7\n",
        "trainSet = np.load('./trainSet.npy')\n",
        "trainSet = torch.from_numpy(trainSet).float()\n",
        "trainLabels = torch.from_numpy(labels[0, :int(234228 * rate)]).to(DEVICE)\n",
        "# del dataMatrix\n",
        "\n",
        "MINIBATCH_SIZE = 128    # mini batch size\n",
        "\n",
        "# first transform the data to dataset can be processed by torch\n",
        "torch_dataset = Data.TensorDataset(trainSet, trainLabels.long())\n",
        "# put the dataset into DataLoader\n",
        "loader = Data.DataLoader(\n",
        "    dataset=torch_dataset,\n",
        "    batch_size=MINIBATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "model = LeNet_II().to(DEVICE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "Epoch = 5\n",
        "model.train()\n",
        "for epoch in range(Epoch):\n",
        "    print(\"epoch %s\" % (epoch+1))\n",
        "    for step, (batch_x, batch_y) in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_x.to(DEVICE))\n",
        "        loss = F.nll_loss(output, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (step + 1) % 50 == 0:\n",
        "            print(\"    step %s: Loss %s\" % (step+1, loss.item()))\n",
        "        if float(loss.item()) <= 0.05:\n",
        "            break\n",
        "\n",
        "testSet = np.load('./testSet.npy')\n",
        "testSet = torch.from_numpy(testSet).float().to(DEVICE)\n",
        "testLabels = torch.from_numpy(labels[0, int(234228 * rate):]).to(DEVICE)\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "\n",
        "MINIBATCH_SIZE = 128    # mini batch size\n",
        "\n",
        "# first transform the data to dataset can be processed by torch\n",
        "torch_dataset = Data.TensorDataset(testSet, testLabels.long())\n",
        "# put the dataset into DataLoader\n",
        "loader = Data.DataLoader(\n",
        "    dataset=torch_dataset,\n",
        "    batch_size=MINIBATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for step, (batch_x, batch_y) in enumerate(loader):\n",
        "        output = model(batch_x)\n",
        "        test_loss += F.nll_loss(output, batch_y, reduction='sum').item()  # sum a batch of loss\n",
        "        predict = output.max(1, keepdim=True)[1]  # find the prediction\n",
        "        correct += predict.eq(batch_y.view_as(predict)).sum().item()\n",
        "        print(\"step %s, test loss: %s\" % (step, test_loss))\n",
        "\n",
        "testSize = testLabels.shape[0]\n",
        "test_loss /= testSize\n",
        "print(\"Test: Average loss:%s, Accuracy: %s/%s (%s)\"\n",
        "      % (test_loss, correct, testSize, correct / testSize))\n",
        "\n"
      ],
      "metadata": {
        "id": "qJSsTsvOFipL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}