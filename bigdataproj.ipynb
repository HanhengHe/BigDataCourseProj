{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "https://gist.github.com/HanhengHe/708386a23e38c0eff0600b900c2258e7#file-bigdataproj-ipynb",
      "authorship_tag": "ABX9TyPY1JfxnAXrnZbGc8nNFr/l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanhengHe/BigDataCourseProj/blob/main/bigdataproj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RUNLARGE = True\n",
        "NUMCLASS= 50 if RUNLARGE else 10\n",
        "dataSize = -1"
      ],
      "metadata": {
        "id": "6JucKGj_8jJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgMFKRmRkdmq",
        "outputId": "3df9e4ac-f7ac-4e12-f7fe-0547ca6c9f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# GET FILE\n",
        "\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=False)\n",
        "\n",
        "filePath = \"/content/drive/MyDrive/Dataset_large\" if RUNLARGE else \"/content/drive/MyDrive/Dataset_small/data1.zip\"\n",
        "\n",
        "with zipfile.ZipFile(filePath,\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"targetdir\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models"
      ],
      "metadata": {
        "id": "h4fFobtta7EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING TRAINING DATA\n",
        "\n",
        "lsTrainLabel = []\n",
        "lsTrainData = []\n",
        "\n",
        "trainingPath = \"/content/targetdir/dataset2\" if RUNLARGE else \"/content/targetdir/data1\"\n",
        "\n",
        "count = 0\n",
        "\n",
        "with open(trainingPath + \"/train_label.txt\") as f:\n",
        "  for line in f.readlines():\n",
        "    expression = line.split(' ')\n",
        "    lsTrainLabel.append(int(expression[1]))\n",
        "    dataRGB = np.load(trainingPath + \"/train/\" + expression[0]).transpose((2,0,1))\n",
        "    lsTrainData.append(dataRGB)\n",
        "    # turn training data into grayscale data\n",
        "    # lsData.append((dataRGB[:, :, 0] + dataRGB[:, :, 1] + dataRGB[:, :, 2]) / 3)\n",
        "    count += 1\n",
        "    if dataSize != -1 and dataSize == count:\n",
        "        break\n",
        "\n",
        "INITER = torch.zeros(2, lsTrainData[0].shape[0], lsTrainData[0].shape[1], lsTrainData[0].shape[2])\n",
        "\n",
        "print(\"Number of traning data: {0}\".format(len(lsTrainLabel)), end='\\n')\n",
        "print(\"Shape of traning data: {0}\".format(lsTrainData[0].shape), end='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBqaS5J8FOYC",
        "outputId": "e8ed4e7a-84de-4ce6-8586-c3fcb39d6643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of traning data: 64163\n",
            "Shape of traning data: (3, 224, 224)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainLabels = np.array(lsTrainLabel)\n",
        "trainSet = np.array(lsTrainData)\n",
        "\n",
        "trainSet = torch.from_numpy(trainSet).float()\n",
        "\n",
        "trainLabels = torch.from_numpy(trainLabels)    # nll\n",
        "torch_dataset = Data.TensorDataset(trainSet, trainLabels.long())    # nll\n",
        "\n",
        "# criterion = nn.MSELoss()  # mse\n",
        "# trainLabels = torch.nn.functional.one_hot(torch.from_numpy(trainLabels), num_classes=NUMCLASS)  # mse\n",
        "# torch_dataset = Data.TensorDataset(trainSet, trainLabels.float())  # mse"
      ],
      "metadata": {
        "id": "JZXLKZYYJcmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CPU = torch.device(\"cpu\")\n",
        "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else CPU\n",
        "# DEVICE = CPU\n",
        "print(\"DEVICE = %s\" % (DEVICE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgcL_FZ8jhi3",
        "outputId": "9421c1cf-658d-4f99-a197-7016be647d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE = cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD TEST DATA\n",
        "\n",
        "lsTestLabel = []\n",
        "lsTestData = []\n",
        "\n",
        "testingPath = \"/content/targetdir/dataset2\" if RUNLARGE else \"/content/targetdir/data1\"\n",
        "\n",
        "\n",
        "with open(testingPath + \"/test_label.txt\") as f:\n",
        "  for line in f.readlines():\n",
        "    expression = line.split(' ')\n",
        "    lsTestLabel.append(int(expression[1]))\n",
        "    dataRGB = np.load(testingPath + \"/test/\" + expression[0]).transpose((2,0,1))\n",
        "    lsTestData.append(dataRGB)\n",
        "    # turn training data into grayscale data\n",
        "    # lsData.append((dataRGB[:, :, 0] + dataRGB[:, :, 1] + dataRGB[:, :, 2]) / 3)\n",
        "\n",
        "print(\"Number of testing data: {0}\".format(len(lsTestLabel)), end='\\n')\n",
        "print(\"Shape of testing data: {0}\".format(lsTestData[0].shape), end='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxehioIAZtM0",
        "outputId": "43364d1c-39bc-41ca-d2e6-3ff251732650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of testing data: 2500\n",
            "Shape of testing data: (3, 224, 224)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUtY0yizrjfb"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, noiseSize=128, ct_deepth=128):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        self.model_noise_channel = nn.Sequential(\n",
        "            nn.ConvTranspose2d(noiseSize, ct_deepth * 8, 4, 1, 0),\n",
        "            nn.BatchNorm2d(ct_deepth * 8, 0.2),\n",
        "            nn.LeakyReLU()\n",
        "            )\n",
        "        \n",
        "        self.model_label_channel = nn.Sequential(\n",
        "            nn.ConvTranspose2d(50, ct_deepth * 8, 4, 1, 0),\n",
        "            nn.BatchNorm2d(ct_deepth * 8, 0.2),\n",
        "            nn.LeakyReLU()\n",
        "            )\n",
        "\n",
        "        self.model_kernal = nn.Sequential(\n",
        "            nn.ConvTranspose2d(ct_deepth * 16, ct_deepth * 12, 4, 2, 1),\n",
        "            nn.BatchNorm2d(ct_deepth * 12, 0.2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose2d(ct_deepth * 12, ct_deepth * 8, 3, 2, 1),\n",
        "            nn.BatchNorm2d(ct_deepth * 8, 0.2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose2d(ct_deepth * 8, ct_deepth * 4, 3, 2, 1),\n",
        "            nn.BatchNorm2d(ct_deepth * 4, 0.2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose2d(ct_deepth * 4, ct_deepth * 2, 3, 2, 1),\n",
        "            nn.BatchNorm2d(ct_deepth * 2, 0.2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose2d(ct_deepth * 2, ct_deepth, 3, 2, 1),\n",
        "            nn.BatchNorm2d(ct_deepth, 0.2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ConvTranspose2d(ct_deepth, 3, 2, 2, 1),\n",
        "            nn.Tanh()\n",
        "            )\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        x_0 = self.model_noise_channel(x)\n",
        "        x_1 = self.model_label_channel(label)\n",
        "        x = torch.cat([x_0, x_1], 1)\n",
        "        x = self.model_kernal(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ct_deepth=128):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.model_noise_channel = nn.Sequential(\n",
        "            nn.Conv2d(3, int(ct_deepth / 2), 4, 1, 2),\n",
        "            nn.BatchNorm2d(int(ct_deepth / 2), 0.2),\n",
        "            nn.LeakyReLU()\n",
        "            )\n",
        "        \n",
        "        self.model_label_channel = nn.Sequential(\n",
        "            nn.Conv2d(50, int(ct_deepth / 2), 4, 1, 2),\n",
        "            nn.BatchNorm2d(int(ct_deepth / 2), 0.2),\n",
        "            nn.LeakyReLU()\n",
        "            )\n",
        "\n",
        "        self.model_kernal = nn.Sequential(\n",
        "            nn.Conv2d(ct_deepth, ct_deepth * 2, 3, 2, 1),\n",
        "            nn.BatchNorm2d(ct_deepth * 2, 0.2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(ct_deepth * 2, ct_deepth * 4, 3, 2, 1),\n",
        "            nn.BatchNorm2d(ct_deepth * 4, 0.2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(ct_deepth * 4, ct_deepth * 2, 4, 4, 1),\n",
        "            nn.BatchNorm2d(ct_deepth * 2, 0.2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(ct_deepth * 2, ct_deepth, 4, 4, 1),\n",
        "            nn.BatchNorm2d(ct_deepth, 0.2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(ct_deepth, 1, 4, 1, 0),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        x_0 = self.model_noise_channel(x)\n",
        "        x_1 = self.model_label_channel(label)\n",
        "        x = torch.cat([x_0, x_1], 1)\n",
        "        x = self.model_kernal(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING MODULE \n",
        "\n",
        "import math\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import time\n",
        "import os.path\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class LogHelper:\n",
        "    def __init__(self):\n",
        "        self.strLog = \"\"\n",
        "    \n",
        "    def PrintAndLog(self, strMsg: str, show=True, end=\"\\n\"):\n",
        "        self.strLog += strMsg\n",
        "        self.strLog += end\n",
        "        if show:\n",
        "            print(strMsg, end=end)\n",
        "\n",
        "Epochs = 20\n",
        "MINLOSS = 1e-5\n",
        "MINIBATCH_SIZE = 32\n",
        "LEARNING_RATE_G = 0.001\n",
        "LEARNING_RATE_D = 0.00002\n",
        "lrChangeFq_G = 10\n",
        "lrChangeFq_D = 5\n",
        "gammaScheduler = 0.79434\n",
        "outputFrequence = 1\n",
        "bSaveFile = True\n",
        "nExtraTrainG_Aid = 5\n",
        "nExtraTrainG_Normal = 1\n",
        "dFirstAid = 0.38\n",
        "\n",
        "logHelper = LogHelper() \n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "loader = Data.DataLoader(\n",
        "    dataset=torch_dataset,\n",
        "    batch_size=MINIBATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "nSteps = math.ceil(trainLabels.shape[0] / MINIBATCH_SIZE)\n",
        "\n",
        "model_G = Generator().to(DEVICE)\n",
        "model_D = Discriminator().to(DEVICE)\n",
        "\n",
        "# OPTIMIZER\n",
        "optimizer_G = optim.Adam(model_G.parameters(), lr=LEARNING_RATE_G)\n",
        "scheduler_G = StepLR(optimizer_G, step_size=lrChangeFq_G, gamma=gammaScheduler)\n",
        "\n",
        "optimizer_D = optim.Adam(model_D.parameters(), lr=LEARNING_RATE_D)\n",
        "scheduler_D = StepLR(optimizer_D, step_size=lrChangeFq_D, gamma=gammaScheduler)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "model_G.train()\n",
        "model_D.train()\n",
        "\n",
        "ls_D_Loss = []\n",
        "ls_G_Loss = []\n",
        "\n",
        "mOnehot = torch.diag(torch.ones(NUMCLASS))[:, :, None, None]\n",
        "\n",
        "fill = torch.zeros([NUMCLASS, NUMCLASS, 224, 224])\n",
        "for i in range(NUMCLASS):\n",
        "    fill[i, i, :, :] = 1\n",
        "\n",
        "struct_time = time.localtime() # get struct_time\n",
        "strTime = time.strftime(\"%m-%d-%Y_%H:%M:%S\", struct_time)\n",
        "\n",
        "fileName = \"CDCGAN_\" + strTime\n",
        "filePath = '/content/drive/MyDrive/BigDataProjLog/CDCGAN/'\n",
        "\n",
        "for epoch in range(Epochs):\n",
        "\n",
        "    print(\"Epoch {0}\".format(epoch+1))\n",
        "\n",
        "    d_CurLoss = 0\n",
        "    g_CurLoss = 0\n",
        "    D_x = 0\n",
        "    for step, (batch_x, batch_y) in enumerate(loader):\n",
        "\n",
        "        model_D.zero_grad()\n",
        "        \n",
        "        # variables used more than once\n",
        "        curOnehot = mOnehot[batch_y].to(DEVICE)\n",
        "        noise = torch.randn((batch_x.shape[0], 128, 1, 1)).to(DEVICE)\n",
        "        filledLabel = fill[batch_y].to(DEVICE)\n",
        "        realLabel = torch.ones((batch_x.shape[0], 1)).to(DEVICE)\n",
        "\n",
        "        # train discriminator\n",
        "        d_ResReal = model_D(batch_x.to(DEVICE), filledLabel)\n",
        "        d_LossReal = criterion(d_ResReal, realLabel)\n",
        "\n",
        "        g_Res = model_G(noise, curOnehot)\n",
        "        d_ResFake = model_D(g_Res, filledLabel)\n",
        "        D_x += d_ResFake.mean().item() / nSteps\n",
        "        d_LossFake = criterion(d_ResFake, torch.zeros((batch_x.shape[0], 1)).to(DEVICE))\n",
        "\n",
        "        d_Loss = d_LossReal + d_LossFake\n",
        "        \n",
        "        bNeedFirstAid = d_ResFake.mean().item() < dFirstAid\n",
        "        if not bNeedFirstAid:\n",
        "            d_Loss.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "        d_CurLoss += d_Loss.item() / nSteps\n",
        "\n",
        "        curExtraTrainG = nExtraTrainG_Aid if bNeedFirstAid else nExtraTrainG_Normal\n",
        "\n",
        "        # train generator\n",
        "        for _ in range(curExtraTrainG):\n",
        "            model_G.zero_grad()\n",
        "\n",
        "            g_Res = model_G(noise, curOnehot)\n",
        "            d_ResTrainG = model_D(g_Res, filledLabel)\n",
        "            d_LossTrainG = criterion(d_ResTrainG, realLabel)  # as real\n",
        "\n",
        "            d_LossTrainG.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            g_CurLoss += d_LossTrainG.item() / nSteps / curExtraTrainG\n",
        "        \n",
        "        # print(\"     [{0}|{1}] G loss: {2:.4}, D loss: {3:.4}, D(x): {4:.4}\".format(step+1, nSteps, d_LossTrainG.item(), d_Loss.item(), d_ResFake.mean().item()))\n",
        "\n",
        "    scheduler_D.step()\n",
        "    LEARNING_RATE_D = optimizer_D.param_groups[0]['lr']\n",
        "\n",
        "    scheduler_G.step()\n",
        "    LEARNING_RATE_G = optimizer_G.param_groups[0]['lr']\n",
        "\n",
        "    ls_G_Loss.append(g_CurLoss)\n",
        "    ls_D_Loss.append(d_CurLoss)\n",
        "\n",
        "    lrMsg = \" lr_G = {0:.5}, lr_D = {1:.5}\".format(LEARNING_RATE_G, LEARNING_RATE_D)\n",
        "    curMsg = \" Epoch {0}, G loss: {1:.4}, D loss: {2:.4}, D(x): {3:.4}\".format(epoch+1, g_CurLoss, d_CurLoss, D_x) + lrMsg\n",
        "\n",
        "    logHelper.PrintAndLog(curMsg, show=((epoch == 0) or ((epoch + 1) % outputFrequence == 0)))\n",
        "\n",
        "    if bSaveFile: \n",
        "        torch.save(model_G.state_dict(), filePath + fileName + \"Generator_E{0}_GL{1}.pth\".format(epoch+1, g_CurLoss))\n",
        "        torch.save(model_D.state_dict(), filePath + fileName + \"Disc_E{0}_GL{1}_DX{2}.pth\".format(epoch+1, d_CurLoss, D_x))\n",
        "\n",
        "if bSaveFile: \n",
        "    if not os.path.exists(filePath):\n",
        "        os.makedirs(filePath)\n",
        "\n",
        "    with open(filePath + fileName + '.log', 'w') as f:\n",
        "        f.write(logHelper.strLog)\n",
        "\n",
        "if bSaveFile: \n",
        "    torch.save(model_G.state_dict(), filePath + fileName + \"Generator.pth\")\n",
        "    torch.save(model_D.state_dict(), filePath + fileName + \"Disc.pth\")\n",
        "\n",
        "# plot\n",
        "plt.figure()\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.xticks([])\n",
        "plt_D, = plt.plot(list(range(0,len(ls_D_Loss))), ls_D_Loss)\n",
        "plt_G, = plt.plot(list(range(0,len(ls_G_Loss))), ls_G_Loss,color='red',linewidth=1.0,linestyle='--')\n",
        "plt.legend(handles=[plt_D,plt_G],labels=['Loss D','Loss G'],loc='best')\n",
        "plt.title(fileName)\n",
        "if bSaveFile: \n",
        "    plt.savefig(filePath + fileName + '.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EHvm5jHFoSpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a42a45-7bb6-4f99-e95a-0326ede9a95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            " Epoch 1, G loss: 1.117, D loss: 0.7789, D(x): 0.3537 lr_G = 0.001, lr_D = 2e-05\n",
            "Epoch 2\n",
            " Epoch 2, G loss: 1.318, D loss: 0.5825, D(x): 0.2787 lr_G = 0.001, lr_D = 2e-05\n",
            "Epoch 3\n",
            " Epoch 3, G loss: 1.332, D loss: 0.5918, D(x): 0.2773 lr_G = 0.001, lr_D = 2e-05\n",
            "Epoch 4\n",
            " Epoch 4, G loss: 1.315, D loss: 0.6011, D(x): 0.2823 lr_G = 0.001, lr_D = 2e-05\n",
            "Epoch 5\n",
            " Epoch 5, G loss: 1.313, D loss: 0.6059, D(x): 0.2838 lr_G = 0.001, lr_D = 1.5887e-05\n",
            "Epoch 6\n",
            " Epoch 6, G loss: 1.348, D loss: 0.5979, D(x): 0.2748 lr_G = 0.001, lr_D = 1.5887e-05\n",
            "Epoch 7\n",
            " Epoch 7, G loss: 1.338, D loss: 0.6038, D(x): 0.2781 lr_G = 0.001, lr_D = 1.5887e-05\n",
            "Epoch 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.resnet import ResNet50_Weights\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.model = models.resnet18()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet34(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet34, self).__init__()\n",
        "        self.model = models.resnet34()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet50, self).__init__()\n",
        "        self.model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet101(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet101, self).__init__()\n",
        "        self.model = models.resnet101()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet152(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet152, self).__init__()\n",
        "        self.model = models.resnet152()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vvisny0Nh2Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.resnet import ResNeXt50_32X4D_Weights, ResNeXt101_32X8D_Weights\n",
        "\n",
        "class ResNeXt50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNeXt50, self).__init__()\n",
        "        self.model = models.resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "class ResNeXt101_32(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNeXt101_32, self).__init__()\n",
        "        self.model = models.resnext101_32x8d(weights=ResNeXt101_32X8D_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "class ResNeXt101_64(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNeXt101_64, self).__init__()\n",
        "        self.model = models.resnext101_64x4d()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mf1inbCpwf0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.densenet import DenseNet121_Weights, DenseNet161_Weights\n",
        "\n",
        "class DenseNet121(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNet121, self).__init__()\n",
        "        self.model = models.densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    def forward(self, x):\n",
        "         x = self.model(x)\n",
        "         x = x.view(x.size(0), -1)\n",
        "         return x\n",
        "\n",
        "class DenseNet161(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNet161, self).__init__()\n",
        "        self.model = models.densenet161(weights=DenseNet161_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    def forward(self, x):\n",
        "         x = self.model(x)\n",
        "         x = x.view(x.size(0), -1)\n",
        "         return x\n",
        "\n",
        "class DenseNet169(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNet169, self).__init__()\n",
        "        self.model = models.densenet169()\n",
        "\n",
        "    def forward(self, x):\n",
        "         x = self.model(x)\n",
        "         x = x.view(x.size(0), -1)\n",
        "         return x\n",
        "\n",
        "class DenseNet201(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNet201, self).__init__()\n",
        "        self.model = models.densenet201()\n",
        "\n",
        "    def forward(self, x):\n",
        "         x = self.model(x)\n",
        "         x = x.view(x.size(0), -1)\n",
        "         return x"
      ],
      "metadata": {
        "id": "TdV-jMy0f9Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.efficientnet import EfficientNet_V2_S_Weights\n",
        "from torchvision.models.convnext import ConvNeXt_Base_Weights\n",
        "\n",
        "class EfficientNetV2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EfficientNetV2, self).__init__()\n",
        "        self.model = models.efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    def forward(self, x):\n",
        "         x = self.model(x)\n",
        "         x = x.view(x.size(0), -1)\n",
        "         return x\n",
        "\n",
        "class ConvNeXt_base(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNeXt_base, self).__init__()\n",
        "        self.model = models.convnext_base(weights=ConvNeXt_Base_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    def forward(self, x):\n",
        "         x = self.model(x)\n",
        "         x = x.view(x.size(0), -1)\n",
        "         return x\n",
        "\n",
        "class ConvNeXt_large(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNeXt_large, self).__init__()\n",
        "        self.model = models.convnext_large()\n",
        "\n",
        "    def forward(self, x):\n",
        "         x = self.model(x)\n",
        "         x = x.view(x.size(0), -1)\n",
        "         return x\n",
        "\n",
        "class MaxVit(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MaxVit, self).__init__()\n",
        "        self.model = models.maxvit()\n",
        "\n",
        "    def forward(self, x):\n",
        "         x = self.model(x)\n",
        "         x = x.view(x.size(0), -1)\n",
        "         return x"
      ],
      "metadata": {
        "id": "44O8lw0yf1rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_dict = {\"ResNet18\": ResNet18,\n",
        "                \"ResNet34\": ResNet34,\n",
        "                \"ResNet50\": ResNet50,\n",
        "                \"ResNet101\": ResNet101,\n",
        "                \"ResNet152\": ResNet152,\n",
        "\n",
        "                \"DenseNet121\": DenseNet121,\n",
        "                \"DenseNet161\": DenseNet161,\n",
        "                \"DenseNet169\": DenseNet169,\n",
        "                \"DenseNet201\": DenseNet201,\n",
        "\n",
        "                \"ResNeXt50\": ResNeXt50,\n",
        "                \"ResNeXt101_32\": ResNeXt101_32,\n",
        "                \"ResNeXt101_64\": ResNeXt101_64,\n",
        "\n",
        "                \"EfficientNetV2\": EfficientNetV2,\n",
        "\n",
        "                \"ConvNeXt_base\": ConvNeXt_base,\n",
        "                \"ConvNeXt_large\": ConvNeXt_large}"
      ],
      "metadata": {
        "id": "eq6T_Xwtf2jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NETWORK DEFINE MODULE\n",
        "\n",
        "torch.cuda._initialized = True\n",
        "\n",
        "class Network(nn.Module):\n",
        "    \n",
        "    def __init__(self, initer: torch.Tensor, num_classes: int, kernalType: str):\n",
        "        \n",
        "        super(Network, self).__init__()\n",
        "        self.backbone = network_dict[kernalType]()\n",
        "        self.fc = nn.Linear(self.backbone(initer).shape[1], num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.backbone(x)\n",
        "        x = self.fc(x)\n",
        "        # x = F.softmax(x, dim=1)  # note this when use ce\n",
        "        return x "
      ],
      "metadata": {
        "id": "F7nr88ZqgQAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING MODULE \n",
        "\n",
        "import math\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import time\n",
        "import os.path\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class LogHelper:\n",
        "    def __init__(self):\n",
        "        self.strLog = \"\"\n",
        "    \n",
        "    def PrintAndLog(self, strMsg: str, show=True, end=\"\\n\"):\n",
        "        self.strLog += strMsg\n",
        "        self.strLog += end\n",
        "        if show:\n",
        "            print(strMsg, end=end)\n",
        "\n",
        "def Processing(KERNALTYPE, \n",
        "               Epochs = 50, \n",
        "               MINLOSS = 1e-5, \n",
        "               MINIBATCH_SIZE = 128, \n",
        "               LEARNING_RATE = 0.001, \n",
        "               lr_diff = 1e-5, \n",
        "               lrChangeFq = -1,\n",
        "               gammaScheduler = 0.8,\n",
        "               outputFrequence = 10\n",
        "               ):\n",
        "\n",
        "    lsTrainLoss = []\n",
        "\n",
        "    logHelper = LogHelper() \n",
        "\n",
        "    bChangeLr = lrChangeFq != -1\n",
        "    torch.cuda.empty_cache()\n",
        "    model = Network(INITER, NUMCLASS, KERNALTYPE).to(DEVICE)\n",
        "\n",
        "    # put the dataset into DataLoader\n",
        "    loader = Data.DataLoader(\n",
        "        dataset=torch_dataset,\n",
        "        batch_size=MINIBATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    nSteps = math.ceil(trainLabels.shape[0] / MINIBATCH_SIZE)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss() # ce\n",
        "    # criterion = nn.NLLLoss()  # nll\n",
        "\n",
        "    # OPTIMIZER\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = StepLR(optimizer, step_size=lrChangeFq if bChangeLr else Epochs, gamma=gammaScheduler)\n",
        "\n",
        "    logHelper.PrintAndLog(\"\\n\\nKERBALTYPE: \" + KERNALTYPE)\n",
        "    logHelper.PrintAndLog(\"INIT LEARNING RATE={0}; EPOCH={1}; CHGFR={2}; GAMMA={3}\".format(LEARNING_RATE, Epochs, lrChangeFq, gammaScheduler))\n",
        "    logHelper.PrintAndLog(\"TRAINING...\")\n",
        "\n",
        "    model.train()\n",
        "    lastLoss = 0\n",
        "    for epoch in range(Epochs):\n",
        "\n",
        "        curLoss = 0\n",
        "\n",
        "        for step, (batch_x, batch_y) in enumerate(loader):\n",
        "            optimizer.zero_grad()        \n",
        "            output = model(batch_x.to(DEVICE))\n",
        "            loss = criterion(output, batch_y.to(DEVICE))  # ce\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            curLoss += loss.item() / nSteps\n",
        "\n",
        "        lsTrainLoss.append(curLoss)\n",
        "\n",
        "        if bChangeLr:\n",
        "            scheduler.step()\n",
        "            LEARNING_RATE = optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        lrMsg = \" lr = {0:.5}\".format(LEARNING_RATE) if bChangeLr else \"\"\n",
        "        curMsg = \" Epoch {0}, general loss: {1}\".format(epoch+1, curLoss) + lrMsg\n",
        "\n",
        "        logHelper.PrintAndLog(curMsg, show=((epoch == 0) or ((epoch + 1) % outputFrequence == 0)))\n",
        "\n",
        "        if curLoss <= MINLOSS:\n",
        "            logHelper.PrintAndLog(\" Terminated with low loss. Epoch {0}, general loss: {1}\".format(epoch+1, curLoss) + lrMsg)\n",
        "            break\n",
        "        \n",
        "        if abs(lastLoss - curLoss) <= lr_diff:\n",
        "            logHelper.PrintAndLog(\" Terminated with unchanged loss. Epoch {0}, general loss: {1}\".format(epoch+1, curLoss) + lrMsg)\n",
        "            break\n",
        "        \n",
        "        lastLoss = curLoss\n",
        "\n",
        "    # TEST MODULE\n",
        "\n",
        "    testLabels = np.array(lsTestLabel)\n",
        "    testSet = np.array(lsTestData)\n",
        "\n",
        "    testSet = torch.from_numpy(testSet).float()\n",
        "    testLabels = torch.from_numpy(testLabels).to(DEVICE)\n",
        "\n",
        "    model.eval()\n",
        "    logHelper.PrintAndLog(\"TESTING...\")\n",
        "\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    # first transform the data to dataset can be processed by torch\n",
        "    # torch_dataset = Data.TensorDataset(testSet, testLabels.long())  # nll\n",
        "\n",
        "    testLabelsOneHot = torch.nn.functional.one_hot(testLabels, num_classes=NUMCLASS)  # mse\n",
        "    torch_dataset = Data.TensorDataset(testSet, testLabelsOneHot.float())  # mse\n",
        "\n",
        "    # put the dataset into DataLoader\n",
        "    loader = Data.DataLoader(\n",
        "        dataset=torch_dataset,\n",
        "        batch_size=MINIBATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=0\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step, (batch_x, batch_y) in enumerate(loader):\n",
        "            output = model(batch_x.to(DEVICE))\n",
        "            test_loss += criterion(output, batch_y.to(DEVICE)).item()  # mse\n",
        "            predict = output.max(1, keepdim=True)[1]  # find the prediction\n",
        "            labels = batch_y.max(1, keepdim=True)[1]\n",
        "            correct += predict.eq(labels.view_as(predict)).sum().item()\n",
        "\n",
        "    testSize = testLabels.shape[0]\n",
        "    test_loss /= testSize\n",
        "    curRate = correct / testSize\n",
        "    logHelper.PrintAndLog(\" Test: Average loss:%s, Accuracy: %s/%s (%s)\"\n",
        "        % (test_loss, correct, testSize, curRate))\n",
        "    \n",
        "    struct_time = time.localtime() # get struct_time\n",
        "    strTime = time.strftime(\"%m-%d-%Y_%H:%M:%S\", struct_time)\n",
        "\n",
        "    fileName = KERNALTYPE + (\"_L_\" if RUNLARGE else \"_S_\") + strTime\n",
        "    filePath = '/content/drive/MyDrive/BigDataProjLog/{0}/'.format(KERNALTYPE)\n",
        "\n",
        "    if not os.path.exists(filePath):\n",
        "        os.makedirs(filePath)\n",
        "\n",
        "    with open(filePath + fileName + '.log', 'w') as f:\n",
        "        f.write(logHelper.strLog)\n",
        "    \n",
        "    torch.save(model.state_dict(), filePath + fileName + \"_ACC={0:.4}.pth\".format(curRate))\n",
        "\n",
        "    # plot\n",
        "    plt.figure()\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xticks([])\n",
        "    plt.plot(list(range(0,len(lsTrainLoss))), lsTrainLoss)\n",
        "    plt.title(fileName)\n",
        "    plt.savefig(filePath + fileName + '.png')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "U6jEzqxzEy4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "\"\"\"for key in network_dict:\n",
        "    Processing(key)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hKjuQO4N1gbg",
        "outputId": "b6dbf250-109d-40fd-9b61-e5478447cf63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for key in network_dict:\\n    Processing(key)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\"ResNet18\", \"ResNet34\", \"ResNet50\", \"ResNet101\", \"ResNet152\", \n",
        "\"DenseNet121\", \"DenseNet161\", \"DenseNet169\", \"DenseNet201\", \n",
        "\"ResNeXt50\", \"ResNeXt101_32\", \"ResNeXt101_64\", \"EfficientNetV2\", \n",
        "\"ConvNeXt_base\", \"ConvNeXt_large\"\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "hxzeNtxQBTJz",
        "outputId": "236f50be-b0fb-4086-b015-9ae08854854b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\"ResNet18\", \"ResNet34\", \"ResNet50\", \"ResNet101\", \"ResNet152\", \\n\"DenseNet121\", \"DenseNet161\", \"DenseNet169\", \"DenseNet201\", \\n\"ResNeXt50\", \"ResNeXt101_32\", \"ResNeXt101_64\", \"EfficientNetV2\", \\n\"ConvNeXt_base\", \"ConvNeXt_large\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Processing(\"ResNet18\", lrChangeFq=5, outputFrequence=10)"
      ],
      "metadata": {
        "id": "eEDJMsg3tWHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "model = Network(INITER, NUMCLASS, \"Vit\").to(DEVICE)\n",
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "qfDsT-QraCm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E5RmxALqhIMO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}